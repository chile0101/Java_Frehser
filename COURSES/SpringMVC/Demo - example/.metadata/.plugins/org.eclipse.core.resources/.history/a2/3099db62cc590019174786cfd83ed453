package project2;




import org.apache.poi.xwpf.usermodel.XWPFParagraph;
import org.apache.poi.xwpf.usermodel.XWPFRun;
import org.apache.poi.xwpf.usermodel.XWPFTable;
import org.apache.xmlbeans.impl.piccolo.io.FileFormatException;

import java.io.FileInputStream;
import java.io.FileNotFoundException;
import java.io.FileOutputStream;
import java.io.IOException;
import java.util.ArrayList;
import java.util.List;

import edu.stanford.nlp.ling.CoreLabel;
import edu.stanford.nlp.ling.HasWord;
import edu.stanford.nlp.pipeline.Annotation;
import edu.stanford.nlp.pipeline.CoreDocument;
import edu.stanford.nlp.pipeline.StanfordCoreNLP;
import edu.stanford.nlp.process.CoreLabelTokenFactory;
import edu.stanford.nlp.process.DocumentPreprocessor;
import edu.stanford.nlp.process.PTBTokenizer;


import java.io.File;
import java.io.FileInputStream;
import java.io.FileNotFoundException;
import java.io.IOException;
import java.io.InputStream;
import java.io.Reader;
import java.io.StringReader;
import java.util.ArrayList;
import java.util.Arrays;
import java.util.LinkedList;
import java.util.List;
import java.util.Properties;

import opennlp.tools.cmdline.postag.POSModelLoader;
//import opennlp.tools.cmdline.postag.POSModelLoader;
import opennlp.tools.namefind.NameFinderME;
import opennlp.tools.namefind.TokenNameFinderModel;
import opennlp.tools.postag.POSModel;
import opennlp.tools.postag.POSSample;
import opennlp.tools.postag.POSTaggerME;
import opennlp.tools.tokenize.SimpleTokenizer;
import opennlp.tools.tokenize.Tokenizer;
import opennlp.tools.tokenize.TokenizerME;
import opennlp.tools.tokenize.TokenizerModel;
import opennlp.tools.tokenize.WhitespaceTokenizer;
import opennlp.tools.util.Span;

import org.apache.poi.xwpf.usermodel.XWPFDocument;
import org.apache.poi.xwpf.usermodel.XWPFTable;
import org.apache.xmlbeans.impl.piccolo.io.FileFormatException;

import java.io.FileInputStream;
import java.io.FileNotFoundException;
import java.io.IOException;
import java.util.ArrayList;
import java.util.List;

public class TableReader1 {
	public static void main(String[] args) {
		//      List<TestCaseBean> list=getTableWord("GGI_US001_Common_Table Synch up.docx");
		//      
		//      for (int i=1;i<list.size();i++)
		//      	
		//      {
		//      	String p = list.get(i).getScenario();
		//          System.out.println(p);
		//      }
	//	findingSentences();
		//detectingPartsOfSpeechExample(); //Parse.java ( lexical analysis)
		//extractingRelationshipsExample();//parse.java ( syntactic analysis)
		
			//LemmaExample1();
		LemmaExample(); // lemma
		
	}
	 private static void LemmaExample1() {
		 
		 	Properties properties = new Properties();
	        properties.put("annotators", "tokenize, ssplit, parse,pos,lemma");
	        StanfordCoreNLP pipeline = new StanfordCoreNLP(properties);
	        
			//StanfordCoreNLP stanfordCoreNLP = Pipeline.getPipeline();
			String text = " when i was child , i used to stay here";
			CoreDocument coreDocument = new CoreDocument(text);
			pipeline.annotate(coreDocument);
			List<CoreLabel> coreLabels = coreDocument.tokens();
			for(CoreLabel coreLabel: coreLabels) {
				String lemma = coreLabel.lemma();
				System.out.println(coreLabel.originalText()+ "="+ lemma);
			}
		}
		
	 
	
	 private static void LemmaExample() {
		

        Properties properties = new Properties();
        properties.put("annotators", "tokenize, ssplit, parse,pos,lemma");
        StanfordCoreNLP pipeline = new StanfordCoreNLP(properties);
        
			
			//String text = " when i was child , i used to stay here";
        List<TestCaseBean> list=getTableWord("GGI_US001_Common_Table Synch up.docx");
		for (int i=1;i<list.size();i++) {
			String p = list.get(i).getScenario()+" "
						+list.get(i).getGiven() +""
						+ list.get(i).getWhen() +""
						+ list.get(i).getThen();
			
       // String paragraph = "The first sentence. (The second sentence).";
        Reader reader = new StringReader(p);
        DocumentPreprocessor documentPreprocessor
                = new DocumentPreprocessor(reader);
        List<String> sentenceList = new LinkedList<String>();
        for (List<HasWord> element : documentPreprocessor) {
            StringBuilder sentence = new StringBuilder();
            List<HasWord> hasWordList = element;
            for (HasWord token : hasWordList) {
                sentence.append(token).append(" ");
            }
            sentenceList.add(sentence.toString());
        }
        System.out.println("-------------------------------------------");
        System.out.println("Row # "+ i+ ": ");
        for (String sentence : sentenceList) {

        	CoreDocument coreDocument = new CoreDocument(sentence);
        	pipeline.annotate(coreDocument);
        	List<CoreLabel> coreLabels = coreDocument.tokens();

        	for(CoreLabel coreLabel: coreLabels) {
        		String lemma = coreLabel.lemma();
        		System.out.println(coreLabel.originalText()+ "="+ lemma);
        	}
        }
		}
	 }
	  private static void extractingRelationshipsExample() {
		  
	        Properties properties = new Properties();
	        properties.put("annotators", "tokenize, ssplit, parse");
	        StanfordCoreNLP pipeline = new StanfordCoreNLP(properties);
	        
	        
	    	List<TestCaseBean> list=getTableWord("GGI_US001_Common_Table Synch up.docx");
			for (int i=1;i<list.size();i++) {
				String p = list.get(i).getScenario()+" "
							+list.get(i).getGiven() +""
							+ list.get(i).getWhen() +""
							+ list.get(i).getThen();
				
	       // String paragraph = "The first sentence. (The second sentence).";
	        Reader reader = new StringReader(p);
	        DocumentPreprocessor documentPreprocessor
	                = new DocumentPreprocessor(reader);
	        List<String> sentenceList = new LinkedList<String>();
	        for (List<HasWord> element : documentPreprocessor) {
	            StringBuilder sentence = new StringBuilder();
	            List<HasWord> hasWordList = element;
	            for (HasWord token : hasWordList) {
	                sentence.append(token).append(" ");
	            }
	            sentenceList.add(sentence.toString());
	        }
	        System.out.println("Row # "+ i+ ": ");
	        for (String sentence : sentenceList) {
		

		
		  Annotation annotation = new Annotation(sentence);
		  pipeline.annotate(annotation);
		  pipeline.prettyPrint(annotation, System.out);
		 

	    }
			}
	  }
	private static void detectingPartsOfSpeechExample() { 
		/*
		 * String sentence = "POS processing is useful for enhancing the " +
		 * "quality of data sent to other elements of a pipeline.";
		 */
		List<TestCaseBean> list=getTableWord("GGI_US001_Common_Table Synch up.docx");
		for (int i=1;i<list.size();i++) {
			String p = list.get(i).getScenario()+". "
						+list.get(i).getGiven() +" ."
						+ list.get(i).getWhen() +"."
						+ list.get(i).getThen();
			
       // String paragraph = "The first sentence. (The second sentence).";
        Reader reader = new StringReader(p);
        DocumentPreprocessor documentPreprocessor
                = new DocumentPreprocessor(reader);
        List<String> sentenceList = new LinkedList<String>();
        for (List<HasWord> element : documentPreprocessor) {
            StringBuilder sentence = new StringBuilder();
            List<HasWord> hasWordList = element;
            for (HasWord token : hasWordList) {
                sentence.append(token).append(" ");
            }
            sentenceList.add(sentence.toString());
        }
        System.out.println("Row # "+ i+ ": ");
        for (String sentence : sentenceList) {
	
			/*
			 * String p = list.get(i).getScenario()+". " +list.get(i).getGiven() +". " +
			 * list.get(i).getWhen() +". " + list.get(i).getThen();
			 */
			
			POSModel model = new POSModelLoader() .load(new
					File("C:\\Users\\training\\Desktop\\NguyenVanSang\\duAn\\projectNLP\\projectNLP", "en-pos-maxent.bin"));
			POSTaggerME tagger = new POSTaggerME(model);

			String tokens[] = WhitespaceTokenizer.INSTANCE .tokenize(sentence);
			String[] tags = tagger.tag(tokens);

//			POSSample sample = new POSSample(tokens, tags); 
//			String posTokens[] = sample.getSentence();
//			String posTags[] = sample.getTags(); 
			
//			for (int j = 0; j< posTokens.length; j++) { 
//				System.out.print( posTokens[j] + " - " + posTags[j]); 
//			} 
			

			for (int j = 0; j < tokens.length; j++) {
				System.out.print(tokens[j] + "[" + tags[j] + "] "); 
				
			}
			System.out.println("\n\n");
		}

	}
	}
	public static List<TestCaseBean> getTableWord(String fileName)
	{
		String TabelRead="";
		List<TestCaseBean> list=new ArrayList<TestCaseBean>();
		TestCaseBean testCaseBean;
		String cell1="",cell2="",cell3="",cell4="";
		try {
			// String fileName = "GGI_US001_Common_SystemAdmin.docx";

			if(!(fileName.endsWith(".doc") || fileName.endsWith(".docx"))) {
				throw new FileFormatException();
			} else {

				XWPFDocument doc = new XWPFDocument(new FileInputStream(fileName));

				List<XWPFTable> table = doc.getTables();

				for (XWPFTable xwpfTable : table) {
					TabelRead +=xwpfTable.getElementType();
					//System.out.println(xwpfTable.getElementType());
					if(TabelRead.toString().equals("TABLETABLETABLE")){
						for (int i = 0; i < xwpfTable.getRows().size(); i++) {

							for (int j = 0; j < xwpfTable.getRow(i).getTableCells().size(); j++) {
								if(j==0)
								{
									cell1=xwpfTable.getRow(i).getCell(j).getText();
								}
								else if(j==1)
								{
									cell2=xwpfTable.getRow(i).getCell(j).getText();
								}
								else if(j==2)
								{
									cell3=xwpfTable.getRow(i).getCell(j).getText();
								}
								else {
									cell4=xwpfTable.getRow(i).getCell(j).getText();
								}

							}
							testCaseBean=new TestCaseBean(cell1,cell2,cell3,cell4);
							list.add(testCaseBean);
						}
					}
					//System.out.println("Total Number of Rows of Table:" + xwpfTable.getNumberOfRows());

				}
			}
		} catch(FileFormatException e) {
			e.printStackTrace();
		} catch (FileNotFoundException e) {
			e.printStackTrace();
		} catch (IOException e) {
			e.printStackTrace();
		}
		return list;
	}
}